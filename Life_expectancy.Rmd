---
title: "Homework 3"
author: "Hajra Shahab"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: lumen
    highlight: pygments
---


```{r}
library(ggplot2)
library(glmnet)
library(leaps)  # needed for regsubsets
library(boot)   # needed for cv.glm
library(knitr)
library(gridExtra)

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

options(scipen = 4)

```


The data use for the next few problems is about life expectancy and comes from [Kaggle](https://www.kaggle.com/kumarajarshi/life-expectancy-who)


### Question 1 - Variable selection

##### **(a)** Save the data file and set the working directory. Run the following code as it is. Look up what `rnorm()` function does. What's contained in the additional columns we append to the original life expectancy dataset?

```{r}
set.seed(95791)

num.noise <- 50
lifexp<-read.csv("life_expentancy_who.csv")

df_lifexp<- data.frame(lifexp, 
                   matrix(rnorm(num.noise * nrow(lifexp)), 
                            nrow = nrow(lifexp)))

# Drop the non-numeric "country" column
df_lifexp = df_lifexp[,-1]

# Remove missing values
df_lifexp<-df_lifexp[complete.cases(df_lifexp), ]


head(df_lifexp)
```

These additional fifty columns, with the help of rnorm(), add columns of multivariate normal random variables in the life expectancy dataset where no. of rows is equivalent to the rows within the existing dataset. Since this is normally distributed, the mean is 0 and standard deviation is 1 for columns generated by rnorm(). We can say that we are adding noise to our dataset. 


##### **(b)** Use the `glm` command to fit a linear regression of `Life_expectancy` on all the other variables in the `df_lifexp` data set.  Print the names of the predictors whose coefficient estimates are statistically significant at the 0.05 level.  Are any of the "noise" predictors statistically significant?

**Hint:** To access the P-value column of a fitted model named `my.fit`, you'll want to look at the `coef(summary(my.fit))` object. 

```{r, cache = TRUE, warning=FALSE}

my.fit <- glm(Life_expectancy ~ ., data = df_lifexp)
names <- coef(summary(my.fit))[,4] < 0.05
coef(my.fit)[names][-1]

```


`Year`, `Status`, `Adult_Mortality`, `infant_deaths`, `Alchohol`, `BMI`, `under_five_deaths`, `Total.expenditure`, `Diphtheria`, `HIV_AIDS`, `Income_comp` and `Schooling`. None of the noise predictors are statistically significant. 


##### **(c)** Use the `cv.glm` command with 10-fold cross-validation to estimate the test error of the model you fit in part (a).  Repeat with number of folds `K = 2, 5, 10, 20, 50, 100` (to make your code more concise and readable, use a loop to iterate over these choices of $K$).  

**Note 1**: This question does NOT ask you to code your CV routine all over again. `cv.glm` automatically handles the CV of the model. You are only asked to try it with different choices of K listed above, which you will use a simple for-loop to do.

**Note 2**: This loop may take a few minutes to run.  I have supplied the option cache = TRUE in the code chunk header to prevent the code from needing to re-execute every time you knit.  This code chunk will re-execute only if the code it contains gets changed. 

```{r, cache = TRUE, warning=FALSE}
set.seed(1)
cv.df.err = cv.glm(df_lifexp, my.fit, K=10)$delta[1]

cv.error = rep(0,6) 
folds = c(2, 5, 10, 20, 50, 100)
for (i in 1:length(folds)){
  cv.error[i] = cv.glm(df_lifexp, my.fit, K = folds[i])$delta[1]
}
cv.error

```


##### **(d)** Calculate the standard deviation of your CV error estimates across 6 different choices of $K$. Then calculate the mean of your CV error estimates. And finally take the ratio of the standard deviation and the mean (i.e. divide the standard deviation by the mean).  This quantity is called the [coefficient of variation](https://en.wikipedia.org/wiki/Coefficient_of_variation).  Do the CV error estimates change much across the different choices of $K$?


```{r}

cv.sd <- sd(cv.error)
cv.mean <- mean(cv.error)
coeff.var <- (cv.sd)/(cv.mean)
coeff.var

```


The `coeff.var` shows the extent of variability in relation to the mean of the population. Hence, we can conclude the CV error estimate does not change much across the different choices of $K$. 
cited from: (https://en.wikipedia.org/wiki/Coefficient_of_variation)


### Best subset selection

##### **(e)**  The code below performs Best Subset Selection to identify which variables in the model are most important.  We only go up to models of size 5, because beyond that the computation time starts to get excessive. 

##### Which variables are included in the best model of each size?  (You will want to work with the `summary(lifexp.subset)` or `coef(lifexp.subset, id = )` object to determine this.)  Are the models all nested?  That is, does the best model of size $k-1$ always a subset of the best model of size $k$?  Do any "noise predictors" appear in any of the models?


```{r, cache = TRUE}
# Best subset selection

lifexp.subset <- regsubsets(Life_expectancy ~ .,
              data = df_lifexp,
               nbest = 1,    # 1 best model for each number of predictors
               nvmax = 5,    # NULL for no limit on number of variables
               method = "exhaustive", really.big = TRUE)

# Add code below to answer the question
subset1 <- coef(lifexp.subset, id = 1)
subset1
subset2 <- coef(lifexp.subset, id = 2)
subset2
subset3 <- coef(lifexp.subset, id = 3)
subset3
subset4 <- coef(lifexp.subset, id = 4)
subset4
subset5 <- coef(lifexp.subset, id = 5)
subset5

```


Subset1: Schooling
Subset2: HIV_AIDS, Schooling
Subset3: Adult_Mortality, HIV_AIDS, and Schooling.
Subset4: Adult_Mortality, HIV_AIDS, Income_comp and Schooling. 
Subset5: Adult_Mortality, Percent_Expenditure, HIV_AIDS, Income_comp and Schooling.
Given this, we can conclude the best of model of size $k-1$ is always a subset of the best model of size $k$, hence the models are all nested. There are no noise predictors in any of the models. 


### Forward Stepwise Selection

##### **(f)**  Modify the code provided in part (e) to perform Forward stepwise selection instead of exhaustive search.  There should be no limit on the maximum size of subset to consider.  You may want to set `really.big = TRUE` when you call `regsubsets()` (search for the online documentation for detail).


```{r}

lifexp.subset.fwd <- regsubsets(Life_expectancy ~ .,
              data = df_lifexp,
               nbest = 1,  # 1 best model for each number of predictors
              nvmax = NULL, 
               method = "forward", really.big = TRUE)
lifexp.subset.fwd
```



##### **(g)** Run `summary()` on the `regsubsets` object you got above and save the output object as `lifexp.summary`. You saw in Lab 3 Part 2(f) that this summary object contains a bunch of useful values such as $R^2$, RSS, AIC, and BIC. Generate the four plots as you did in the lab and indicate the *"optimal"* models on each of the curves using `geom_point.` Interpret your results.


```{r}
lifexp.summary <- summary(lifexp.subset.fwd)
num_variables<-seq(1,length(lifexp.summary$rss))

plot_RSQ<-ggplot(data = data.frame(lifexp.summary$rsq),
                 aes(x=num_variables,y=lifexp.summary$rsq))+
  geom_line()+
  geom_point(x=which.max(lifexp.summary$rsq),
             y=max(lifexp.summary$rsq),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("R-squared")+
  theme_bw()


plot_RSS<-ggplot(data = data.frame(lifexp.summary$rss),
                 aes(x=num_variables,y=lifexp.summary$rss))+
  geom_line()+
  geom_point(x=which.min(lifexp.summary$rss),
             y=min(lifexp.summary$rss),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("RSS")+
  theme_bw()

plot_BIC<-ggplot(data = data.frame(lifexp.summary$bic),
                 aes(x=num_variables,y=lifexp.summary.bic))+
  geom_line()+
  geom_point(x=which.min(lifexp.summary$bic),
             y=min(lifexp.summary$bic),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("BIC")+
  theme_bw()


plot_AIC<-ggplot(data = data.frame(lifexp.summary$cp),
                 aes(x=num_variables,y=lifexp.summary.cp))+
  geom_line()+
  geom_point(x=which.min(lifexp.summary$cp),
             y=min(lifexp.summary$cp),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("AIC")+
  theme_bw()

grid.arrange(plot_RSQ, plot_RSS, plot_AIC,plot_BIC, ncol=2,nrow=2)
```


For RSQ, we should pick the model with maximum R-squared at ~ 0.84 and number of variables = 70. For RSS, AIC, and BIC, the best fit models should come from their minimum points. For RSS, at 70 variables, the value is 20,000. For AIC, at ~21 variables, the value is 0. For BIC, at 12 variables, the BIC drops to ~ -600. 


### Question 2 - Lasso

> For the Lasso problems below, start by reading carefully through the code examples in the [Linear regression with glmnet](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#lin) vignette up until the "Linear Regression" section.  Running the glmnet command `glmnet(x = X, y = y)` where `y` is your response vector and `X` is your covariates matrix will fit a Lasso (the default penalty term parameter is `alpha = 1`).  

##### **(a)** Use the `glmnet` command to fit a Lasso to this data. Call the result `lifexp.lasso`.  For this task let's not manually specify the range grid of lambda as we did in the lab, but instead let `glmnet` automatically determine the range of lambda values to fit.  At the end, display the coefficients of the model with `lambda` value $0.1$.

```{r}

x <- model.matrix(Life_expectancy~., data = df_lifexp)[,-1]
y <- df_lifexp$Life_expectancy

lifexp.lasso <- glmnet(x = x, y = y, alpha = 1)
coef(lifexp.lasso, s = 0.1)

```


##### **(b)** It turns out that `lifexp.lasso` contains model fits for an entire sequence of $\lambda$ values.  Look at the `lifexp.lasso$lambda` attribute.  How many $\lambda$ values do we have model fits for?

```{r}

length(lifexp.lasso$lambda)

```


We have model fits for 81 $\lambda$ values


##### **(c)**  Run the `plot` command on your `lifexp.lasso` object to get a regularization plot.  Review the help file for `plot.glmnet` to figure out how to set "norm" as the x-axis variable option, and how to add labels to the curves.  In this parameterization of the x-axis, as the x-axis variable increases (i.e. as we move from the left to the right on the plot), is the corresponding model getting more complex or less complex ?

```{r}

plot(lifexp.lasso, xvar = c("norm"), label = TRUE)

```


As x variable increases, the corresponding model becomes more complex. 


##### Now set the x-axis variable to each of the other two options available and redo the plot. In these two plots, as the x-axis variable increases (i.e. as we move from the left to the right on the plot), is the corresponding model getting more complex or less complex ?

```{r}

plot(lifexp.lasso, xvar = c("lambda"), label = TRUE)
plot(lifexp.lasso, xvar = c("dev"), label = TRUE)

```

As x variable increases, the corresponding model becomes more complex when we set x-axis as `fraction deviance`. However, as x variable increases, the corresponding model becomes less complex for `lambda`. 


##### **(d)**  Perform cross-validation using `cv.glmnet`.  Use the `plot` command to construct a CV error curve.

```{r}
set.seed(1)
cv.out = cv.glmnet(x, y, alpha = 1, lamda = grid)
plot(cv.out)

```


##### **(e)** What is the 1-SE rule choice of $\lambda$? Display the non-zero coefficients in the model with this 1-SE choice of $\lambda$.

```{r}

bestlam.1se <- cv.out$lambda.1se
bestlam.1se

lasso.coef = predict(cv.out, s = bestlam.1se, type = "coefficients")
lasso.coef

```


- The responses which have a `.` in the answer shows that we have a zero coefficient for that variable. 

##### **(f)** The `glmnet()` function has a parameter `standardize` which is by default set to `TRUE`. With `standardize = TRUE`, `glmnet()` standardizes (scales) all x variables to have unit variance (i.e. variance=1) before fitting the model (after model fitting, the coefficient estimates are then converted back to the original x variable scales for reporting). Let's fit a new Lasso model called `lifexp.lasso.ns` just like how we fit `lifexp.lasso` but this time with `standardize = FALSE`. Display the coefficients of `lifexp.lasso.ns` with the same $\lambda$ value from Part (e). Which coefficients are non-zero?

```{r}

x <- model.matrix(Life_expectancy~., data = df_lifexp)[,-1]
y <- df_lifexp$Life_expectancy

lifexp.lasso.ns <- glmnet(x = x, y = y, alpha = 1, standardize = FALSE)

lasso.coef.ns = predict(lifexp.lasso.ns, s = bestlam.1se, type = "coefficients")
lasso.coef.ns

```


- Intercept and Population have non-zero coefficients. 

##### Why do we usually need to standardize the data before fitting a regularized regression model? (Hint: the answer is in ISLR &sect;6.2.1 - although ISLR talks about the rationale of standardization in the context of ridge regression, the same is applicable to Lasso too.)

It is best to standardize the data before fitting a regularized regression model because lasso generally favors large absolute values which creates a bias in our model hence it is important to standardize to eliminate this bias and bring all variables on the same scale. As a result the final fit will not depend on the scale on which the predictors are measured (referred from book).



